spark-submit --driver-memory 15g MovieLensALS.py --spark-executor-memory 15g --local-threads 8 --num-partitions 7 --checkpoint-dir /home/maxikov/spark_dir --temp-dir /home/maxikov/spark_dir --persist-dir /home/maxikov/product_regression_all_regression_tree_rank_12_depth_5.state --data-path datasets/ml-20m --movies-file datasets/ml-20m/ml-20m.imdb.medium.csv --tvtropes-file datasets/dbtropes/tropes.csv --rank 12 --lmbda 0.01 --num-iter 300 --predict-product-features --metadata-sources years genres average_rating imdb_keywords imdb_producer imdb_director tags tvtropes --drop-rare-features 250 --drop-rare-movies 50 --cross-validation 70 --regression-model regression_tree --nbins 32
2017-05-16 10:53:21,084 - __main__ - DEBUG - rank: 12, lmbda: 0.01, num_iter: 300, num_partitions: 7
2017-05-16 10:53:21,084 - __main__ - DEBUG - data_path: datasets/ml-20m, checkpoint_dir: /home/maxikov/spark_dir
2017-05-16 10:53:21,084 - __main__ - DEBUG - Temp dir: /home/maxikov/spark_dir
2017-05-16 10:53:21,084 - __main__ - DEBUG - local_threads: 8
2017-05-16 10:53:21,084 - __main__ - DEBUG - spark_executor_memory: 15g
2017-05-16 10:53:21,085 - __main__ - DEBUG - regression_model: regression_tree
2017-05-16 10:53:21,085 - __main__ - DEBUG - nbins: 32
2017-05-16 10:53:21,085 - __main__ - DEBUG - regression_users: False
2017-05-16 10:53:21,085 - __main__ - DEBUG - predict_product_features: True
2017-05-16 10:53:21,085 - __main__ - DEBUG - metadata_sources: ['years', 'genres', 'average_rating', 'imdb_keywords', 'imdb_producer', 'imdb_director', 'tags', 'tvtropes']
2017-05-16 10:53:21,085 - __main__ - DEBUG - movies_file: datasets/ml-20m/ml-20m.imdb.medium.csv
2017-05-16 10:53:21,085 - __main__ - DEBUG - cross_validation: 70
2017-05-16 10:53:21,086 - __main__ - DEBUG - tvtropes_file: datasets/dbtropes/tropes.csv
2017-05-16 10:53:21,086 - __main__ - DEBUG - features_trim_percentile: 0
2017-05-16 10:53:21,086 - __main__ - DEBUG - drop_missing_movies: False
2017-05-16 10:53:21,086 - __main__ - DEBUG - drop_rare_features: 250
2017-05-16 10:53:21,086 - __main__ - DEBUG - filter_data_set: 10
2017-05-16 10:53:21,086 - __main__ - DEBUG - persist_dir: /home/maxikov/product_regression_all_regression_tree_rank_12_depth_5.state, override_args: False
2017-05-16 10:53:21,086 - __main__ - DEBUG - drop_rare_movies: 50
2017-05-16 10:53:21,086 - __main__ - DEBUG - normalize: False
2017-05-16 10:53:21,087 - __main__ - DEBUG - max_depth: None
2017-05-16 10:53:21,087 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_12_depth_5.state/args.pkl not found, loading new
2017-05-16 10:53:21,087 - __main__ - DEBUG - Storing in /home/maxikov/product_regression_all_regression_tree_rank_12_depth_5.state/args.pkl
2017-05-16 10:53:22,667 - __main__ - DEBUG - msep: ,
2017-05-16 10:53:22,667 - __main__ - DEBUG - Loading ratings
2017-05-16 10:53:35,330 - __main__ - DEBUG - Done in 12.662598 seconds
2017-05-16 10:53:35,330 - __main__ - DEBUG - Loading movies
2017-05-16 10:53:36,883 - __main__ - DEBUG - Done in 1.552869 seconds
2017-05-16 10:53:36,883 - __main__ - DEBUG - 26804 movies loaded
2017-05-16 10:54:39,508 - __main__ - DEBUG - 19912669 records in the training set
2017-05-16 10:54:49,008 - __main__ - DEBUG - 26284 unique movies in the training set
2017-05-16 10:54:49,008 - __main__ - DEBUG - Started internal_feature_predictor
2017-05-16 10:54:49,008 - __main__ - DEBUG - Trying to load previous results
2017-05-16 10:54:49,008 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_12_depth_5.state/results.pkl not found
2017-05-16 10:54:49,008 - __main__ - DEBUG - Training the average rating model
2017-05-16 10:55:09,950 - __main__ - DEBUG - Done in 20.941367 seconds
2017-05-16 10:55:09,980 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_12_depth_5.state/als_model.pkl not found, bulding a new model
2017-05-16 10:55:09,980 - __main__ - DEBUG - Training ALS recommender
2017-05-16 11:12:02,276 - __main__ - DEBUG - Done in 1012.296335 seconds
2017-05-16 11:12:02,277 - __main__ - DEBUG - Saving model to /home/maxikov/product_regression_all_regression_tree_rank_12_depth_5.state/als_model.pkl
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:04 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,894,275
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,894,381
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,894,197
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,894,639
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,894,435
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,894,292
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,894,409
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,189B for [id] INT32: 19,785 values, 79,140B raw, 79,150B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,185B for [id] INT32: 19,784 values, 79,136B raw, 79,146B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,188B for [id] INT32: 19,785 values, 79,140B raw, 79,149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,184B for [id] INT32: 19,784 values, 79,136B raw, 79,145B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,188B for [id] INT32: 19,785 values, 79,140B raw, 79,149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,458,319B for [features, list, element] DOUBLE: 237,420 values, 1,948,847B raw, 1,458,224B comp, 2 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,458,537B for [features, list, element] DOUBLE: 237,408 values, 1,948,749B raw, 1,458,442B comp, 2 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,188B for [id] INT32: 19,785 values, 79,140B raw, 79,149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,458,023B for [features, list, element] DOUBLE: 237,420 values, 1,948,847B raw, 1,457,928B comp, 2 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,457,778B for [features, list, element] DOUBLE: 237,408 values, 1,948,749B raw, 1,457,683B comp, 2 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,457,854B for [features, list, element] DOUBLE: 237,420 values, 1,948,847B raw, 1,457,759B comp, 2 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,188B for [id] INT32: 19,785 values, 79,140B raw, 79,149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,457,954B for [features, list, element] DOUBLE: 237,420 values, 1,948,847B raw, 1,457,859B comp, 2 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,458,279B for [features, list, element] DOUBLE: 237,420 values, 1,948,847B raw, 1,458,184B comp, 2 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 16, 2017 11:12:05 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 570,720
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 571,472
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,075B for [id] INT32: 3,758 values, 15,032B raw, 15,037B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 570,696
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 269,685B for [features, list, element] DOUBLE: 45,096 values, 370,175B raw, 269,638B comp, 1 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 557,392
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,107B for [id] INT32: 3,766 values, 15,064B raw, 15,069B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 268,759B for [features, list, element] DOUBLE: 45,192 values, 370,963B raw, 268,712B comp, 1 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 575,376
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 567,968
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,099B for [id] INT32: 3,764 values, 15,056B raw, 15,061B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 14,691B for [id] INT32: 3,662 values, 14,648B raw, 14,653B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.InternalParque2017-05-16 11:12:06,088 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_12_depth_5.state/baseline_predictions.pkl not found, building new predictions
2017-05-16 11:12:06,088 - __main__ - DEBUG - Computing model predictions
2017-05-16 11:12:19,142 - __main__ - DEBUG - Done in 13.053750 seconds
2017-05-16 11:12:19,142 - __main__ - DEBUG - Computing mean error
2017-05-16 11:19:11,324 - __main__ - DEBUG - Done in 412.181669 seconds
2017-05-16 11:19:11,324 - __main__ - DEBUG - Mean error: 0.53592674738, RMSE: 0.702557959407
2017-05-16 11:19:11,347 - __main__ - DEBUG - Original recommender Evaluating the model
2017-05-16 11:19:11,373 - __main__ - DEBUG - Original recommender Bin range: (0.0, 5.5)
Traceback (most recent call last):
  File "/mnt/encrypted_data/Dropbox/cmu/foundation_of_privacy/project/fop_qii/MovieLensALS.py", line 491, in <module>
tRecordWriter: Flushing mem columnStore to file. allocated memory: 573,832
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 262,765B for [features, list, element] DOUBLE: 43,944 values, 360,719B raw, 262,718B comp, 1 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 269,898B for [features, list, element] DOUBLE: 45,168 values, 370,766B raw, 269,851B comp, 1 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,032B for [id] INT32: 3,747 values, 14,988B raw, 14,994B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,291B for [id] INT32: 3,812 values, 15,248B raw, 15,253B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 268,265B for [features, list, element] DOUBLE: 44,964 values, 369,092B raw, 268,218B comp, 1 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 271,655B for [features, list, element] DOUBLE: 45,744 values, 375,494B raw, 271,608B comp, 1 pages, encodings: [RLE, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,143B for [id] INT32: 3,775 values, 15,100B raw, 15,105B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 16, 2017 11:12:06 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 271,324B for [features, list, element] DOUBLE: 45,300 values, 371,850B raw, 271,277B comp, 1 pages, encodings: [RLE, PLAIN]
    main()
----------------------------------------
Exception happened during processing of request from ('127.0.0.1', 36946)
Traceback (most recent call last):
  File "/usr/lib/python2.7/SocketServer.py", line 290, in _handle_request_noblock
    self.process_request(request, client_address)
  File "/usr/lib/python2.7/SocketServer.py", line 318, in process_request
    self.finish_request(request, client_address)
  File "/usr/lib/python2.7/SocketServer.py", line 331, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File "/usr/lib/python2.7/SocketServer.py", line 652, in __init__
    self.handle()
  File "/opt/spark-2.0.1/python/lib/pyspark.zip/pyspark/accumulators.py", line 233, in handle
    r, _, _ = select.select([self.rfile], [], [], 1)
error: (4, 'Interrupted system call')
----------------------------------------
