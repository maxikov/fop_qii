spark-submit --driver-memory 15g MovieLensALS.py --spark-executor-memory 15g --local-threads 8 --num-partitions 7 --checkpoint-dir /home/maxikov/spark_dir --temp-dir /home/maxikov/spark_dir --persist-dir /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state --data-path datasets/ml-20m --movies-file datasets/ml-20m/ml-20m.imdb.medium.csv --tvtropes-file datasets/dbtropes/tropes.csv --rank 40 --lmbda 0.01 --num-iter 300 --predict-product-features --metadata-sources years genres average_rating imdb_keywords imdb_producer imdb_director tags tvtropes --drop-rare-features 250 --drop-rare-movies 50 --cross-validation 70 --regression-model regression_tree --nbins 32 --max-depth 5 --features-trim-percentile 90
2017-05-19 23:33:44,969 - __main__ - DEBUG - rank: 40, lmbda: 0.01, num_iter: 300, num_partitions: 7
2017-05-19 23:33:44,970 - __main__ - DEBUG - data_path: datasets/ml-20m, checkpoint_dir: /home/maxikov/spark_dir
2017-05-19 23:33:44,970 - __main__ - DEBUG - Temp dir: /home/maxikov/spark_dir
2017-05-19 23:33:44,970 - __main__ - DEBUG - local_threads: 8
2017-05-19 23:33:44,970 - __main__ - DEBUG - spark_executor_memory: 15g
2017-05-19 23:33:44,970 - __main__ - DEBUG - regression_model: regression_tree
2017-05-19 23:33:44,970 - __main__ - DEBUG - nbins: 32
2017-05-19 23:33:44,970 - __main__ - DEBUG - regression_users: False
2017-05-19 23:33:44,970 - __main__ - DEBUG - predict_product_features: True
2017-05-19 23:33:44,970 - __main__ - DEBUG - metadata_sources: ['years', 'genres', 'average_rating', 'imdb_keywords', 'imdb_producer', 'imdb_director', 'tags', 'tvtropes']
2017-05-19 23:33:44,970 - __main__ - DEBUG - movies_file: datasets/ml-20m/ml-20m.imdb.medium.csv
2017-05-19 23:33:44,970 - __main__ - DEBUG - cross_validation: 70
2017-05-19 23:33:44,970 - __main__ - DEBUG - tvtropes_file: datasets/dbtropes/tropes.csv
2017-05-19 23:33:44,970 - __main__ - DEBUG - features_trim_percentile: 90
2017-05-19 23:33:44,970 - __main__ - DEBUG - drop_missing_movies: False
2017-05-19 23:33:44,970 - __main__ - DEBUG - drop_rare_features: 250
2017-05-19 23:33:44,970 - __main__ - DEBUG - filter_data_set: 10
2017-05-19 23:33:44,970 - __main__ - DEBUG - persist_dir: /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state, override_args: False
2017-05-19 23:33:44,970 - __main__ - DEBUG - drop_rare_movies: 50
2017-05-19 23:33:44,970 - __main__ - DEBUG - normalize: False
2017-05-19 23:33:44,970 - __main__ - DEBUG - max_depth: 5
2017-05-19 23:33:44,970 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/args.pkl not found, loading new
2017-05-19 23:33:44,971 - __main__ - DEBUG - Storing in /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/args.pkl
2017-05-19 23:33:46,271 - __main__ - DEBUG - msep: ,
2017-05-19 23:33:46,271 - __main__ - DEBUG - Loading ratings
2017-05-19 23:33:59,216 - __main__ - DEBUG - Done in 12.944528 seconds
2017-05-19 23:33:59,216 - __main__ - DEBUG - Loading movies
2017-05-19 23:34:00,675 - __main__ - DEBUG - Done in 1.458331 seconds
2017-05-19 23:34:00,675 - __main__ - DEBUG - 26804 movies loaded
2017-05-19 23:35:10,786 - __main__ - DEBUG - 19912669 records in the training set
2017-05-19 23:35:17,707 - __main__ - DEBUG - 26284 unique movies in the training set
2017-05-19 23:35:17,707 - __main__ - DEBUG - Started internal_feature_predictor
2017-05-19 23:35:17,707 - __main__ - DEBUG - Trying to load previous results
2017-05-19 23:35:17,707 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/results.pkl not found
2017-05-19 23:35:17,707 - __main__ - DEBUG - Training the average rating model
2017-05-19 23:35:41,556 - __main__ - DEBUG - Done in 23.848975 seconds
2017-05-19 23:35:41,585 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/als_model.pkl not found, bulding a new model
2017-05-19 23:35:41,585 - __main__ - DEBUG - Training ALS recommender
2017-05-20 00:44:45,231 - __main__ - DEBUG - Done in 4143.646143 seconds
2017-05-20 00:44:45,232 - __main__ - DEBUG - Saving model to /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/als_model.pkl
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:47 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,080,777
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,079,935
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,079,781
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,079,539
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,185B for [id] INT32: 19,784 values, 79,136B raw, 79,146B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,188B for [id] INT32: 19,785 values, 79,140B raw, 79,149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,189B for [id] INT32: 19,785 values, 79,140B raw, 79,150B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,080,498
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,188B for [id] INT32: 19,785 values, 79,140B raw, 79,149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,079,718
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,079,218
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,188B for [id] INT32: 19,785 values, 79,140B raw, 79,149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,188B for [id] INT32: 19,785 values, 79,140B raw, 79,149B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,861,836B for [features, list, element] DOUBLE: 791,360 values, 6,410,106B raw, 4,861,501B comp, 7 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,862,873B for [features, list, element] DOUBLE: 791,400 values, 6,410,430B raw, 4,862,538B comp, 7 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79,184B for [id] INT32: 19,784 values, 79,136B raw, 79,145B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,861,844B for [features, list, element] DOUBLE: 791,400 values, 6,410,430B raw, 4,861,509B comp, 7 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,861,678B for [features, list, element] DOUBLE: 791,400 values, 6,410,430B raw, 4,861,343B comp, 7 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,862,709B for [features, list, element] DOUBLE: 791,400 values, 6,410,430B raw, 4,862,374B comp, 7 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,861,781B for [features, list, element] DOUBLE: 791,400 values, 6,410,430B raw, 4,861,446B comp, 7 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 4,861,098B for [features, list, element] DOUBLE: 791,360 values, 6,410,106B raw, 4,860,763B comp, 7 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
May 20, 2017 12:44:48 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,119,640
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,107B for [id] INT32: 3,766 values, 15,064B raw, 15,069B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 906,315B for [features, list, element] DOUBLE: 150,640 values, 1,220,209B raw, 906,220B comp, 2 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,121,555
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,143B for [id] INT32: 3,775 values, 15,100B raw, 15,105B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 910,992B for [features, list, element] DOUBLE: 151,000 values, 1,223,125B raw, 910,897B comp, 2 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,120,350
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,118,316
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,099B for [id] INT32: 3,764 values, 15,056B raw, 15,061B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,075B for [id] INT32: 3,758 values, 15,032B raw, 15,037B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 908,500B for [features, list, element] DOUBLE: 150,560 values, 1,219,561B raw, 908,405B comp, 2 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated 2017-05-20 00:44:49,190 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/baseline_predictions.pkl not found, building new predictions
2017-05-20 00:44:49,190 - __main__ - DEBUG - Computing model predictions
2017-05-20 00:45:01,990 - __main__ - DEBUG - Done in 12.799760 seconds
2017-05-20 00:45:01,990 - __main__ - DEBUG - Computing mean error
2017-05-20 00:51:48,222 - __main__ - DEBUG - Done in 406.232225 seconds
2017-05-20 00:51:48,223 - __main__ - DEBUG - Mean error: 0.436568644216, RMSE: 0.584874029506
2017-05-20 00:51:48,260 - __main__ - DEBUG - Original recommender Evaluating the model
2017-05-20 00:51:48,304 - __main__ - DEBUG - Original recommender Bin range: (0.0, 5.5)
2017-05-20 01:05:04,691 - __main__ - DEBUG - Done in 796.430043 seconds
2017-05-20 01:05:04,691 - __main__ - DEBUG - Original recommender Mean error: -0.0101024826892, mean absolute error: 0.436568644216
2017-05-20 01:07:29,162 - __main__ - DEBUG - Original recommender RMSE: 0.584874029506, variance explained: 0.694098650789, mean absolute error: 0.436568644216, r2: 0.690737589059
2017-05-20 01:07:29,162 - __main__ - DEBUG - Original recommender MRAE: 0.17360353572
2017-05-20 01:07:29,162 - __main__ - DEBUG - Original recommender Errors histogram: ([-5.5, -5.166666666666667, -4.833333333333333, -4.5, -4.166666666666667, -3.8333333333333335, -3.5, -3.166666666666667, -2.8333333333333335, -2.5, -2.166666666666667, -1.8333333333333335, -1.5, -1.166666666666667, -0.8333333333333339, -0.5, -0.16666666666666696, 0.16666666666666607, 0.5, 0.833333333333333, 1.166666666666666, 1.5, 1.833333333333333, 2.166666666666666, 2.5, 2.833333333333332, 3.166666666666666, 3.5, 3.833333333333332, 4.166666666666666, 4.5, 4.833333333333332, 5.166666666666666, 5.5], [0, 0, 0, 0, 5, 40, 149, 448, 1741, 6546, 24867, 91316, 304854, 895514, 2173503, 4149178, 5608888, 3470044, 1722663, 797204, 366599, 168301, 77147, 33582, 13457, 4821, 1390, 348, 51, 12, 1, 0, 0])
2017-05-20 01:07:29,162 - __main__ - DEBUG - Original recommender Absolute errors histogram: ([0.0, 0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.8333333333333333, 1.0, 1.1666666666666665, 1.3333333333333333, 1.5, 1.6666666666666665, 1.8333333333333333, 2.0, 2.1666666666666665, 2.333333333333333, 2.5, 2.6666666666666665, 2.833333333333333, 3.0, 3.1666666666666665, 3.333333333333333, 3.5, 3.6666666666666665, 3.833333333333333, 4.0, 4.166666666666666, 4.333333333333333, 4.5, 4.666666666666666, 4.833333333333333, 5.0, 5.166666666666666, 5.333333333333333, 5.5], [5608888, 4356654, 3262568, 2317995, 1578171, 1033043, 659675, 414514, 256939, 159834, 99783, 62798, 39216, 24701, 15427, 9376, 5822, 3355, 1914, 1001, 538, 264, 124, 39, 17, 8, 4, 1, 0, 0, 0, 0, 0])
2017-05-20 01:07:29,162 - __main__ - DEBUG - Original recommender Squared errors histogram: ([0.0, 0.9166666666666666, 1.8333333333333333, 2.75, 3.6666666666666665, 4.583333333333333, 5.5, 6.416666666666666, 7.333333333333333, 8.25, 9.166666666666666, 10.083333333333332, 11.0, 11.916666666666666, 12.833333333333332, 13.75, 14.666666666666666, 15.583333333333332, 16.5, 17.416666666666664, 18.333333333333332, 19.25, 20.166666666666664, 21.083333333333332, 22.0, 22.916666666666664, 23.833333333333332, 24.75, 25.666666666666664, 26.583333333333332, 27.5, 28.416666666666664, 29.333333333333332, 30.25], [17934888, 1335512, 371535, 140441, 62768, 31027, 16260, 8889, 5044, 2812, 1572, 841, 489, 291, 140, 91, 31, 18, 7, 3, 6, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]):
2017-05-20 01:07:29,163 - __main__ - DEBUG - Original recommender Predictions histogram: ([0.0, 0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.8333333333333333, 1.0, 1.1666666666666665, 1.3333333333333333, 1.5, 1.6666666666666665, 1.8333333333333333, 2.0, 2.1666666666666665, 2.333333333333333, 2.5, 2.6666666666666665, 2.833333333333333, 3.0, 3.1666666666666665, 3.333333333333333, 3.5, 3.6666666666666665, 3.833333333333333, 4.0, 4.166666666666666, 4.333333333333333, 4.5, 4.666666666666666, 4.833333333333333, 5.0, 5.166666666666666, 5.333333333333333, 5.5], [3297, 6552, 14474, 30192, 36709, 52750, 83056, 96816, 115583, 147380, 182690, 244120, 334393, 398220, 497883, 629538, 794418, 1068484, 1304625, 1384113, 1531974, 1632141, 1697348, 1775268, 1558529, 1248440, 997932, 741423, 562888, 437984, 198172, 69744, 22838])
2017-05-20 01:07:29,163 - __main__ - DEBUG - Original recommender Observations histogram: ([0.0, 0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.8333333333333333, 1.0, 1.1666666666666665, 1.3333333333333333, 1.5, 1.6666666666666665, 1.8333333333333333, 2.0, 2.1666666666666665, 2.333333333333333, 2.5, 2.6666666666666665, 2.833333333333333, 3.0, 3.1666666666666665, 3.333333333333333, 3.5, 3.6666666666666665, 3.833333333333333, 4.0, 4.166666666666666, 4.333333333333333, 4.5, 4.666666666666666, 4.833333333333333, 5.0, 5.166666666666666, 5.333333333333333, 5.5], [0, 0, 0, 237647, 0, 0, 677127, 0, 0, 278250, 0, 0, 1424879, 0, 0, 880397, 0, 0, 4273372, 0, 0, 2192341, 0, 0, 5536793, 0, 0, 1528880, 0, 0, 2882983, 0, 0])
2017-05-20 01:07:29,164 - __main__ - DEBUG - Writing /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/baseline_predictions.pkl
2017-05-20 01:15:34,150 - __main__ - DEBUG - Writing /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/results.pkl
2017-05-20 01:15:35,729 - __main__ - DEBUG - AAA  baseline_predictions, features: {}
2017-05-20 01:15:35,745 - __main__ - DEBUG - /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/trimmed_recommender.pkl or /home/maxikov/product_regression_all_regression_tree_rank_40_depth_5_features_trim_percentile_90.state/results.pkl not found, bulding a new model
2017-05-20 01:15:35,745 - __main__ - DEBUG - Training trimmed recommender
2017-05-20 01:15:35,745 - __main__ - DEBUG - Trimming feature distributions to leave 90% of data
2017-05-20 01:15:35,745 - __main__ - DEBUG - Processing feature 0
2017-05-20 01:15:36,879 - __main__ - DEBUG - 90% of data are between -1.14359924793 and 0.0501334309578, thresholding the rest
2017-05-20 01:15:37,099 - __main__ - DEBUG - 90% of data are between -0.864282029867 and 0.159130470455, thresholding the rest
2017-05-20 01:15:37,099 - __main__ - DEBUG - Processing feature 1
2017-05-20 01:15:38,324 - __main__ - DEBUG - 90% of data are between -0.724089205265 and 0.534875643253, thresholding the rest
2017-05-20 01:15:38,595 - __main__ - DEBUG - 90% of data are between -0.669068175554 and 0.338324230909, thresholding the rest
2017-05-20 01:15:38,595 - __main__ - DEBUG - Processing feature 2
2017-05-20 01:15:40,187 - __main__ - DEBUG - 90% of data are between -0.777291619778 and 0.456490761042, thresholding the rest
2017-05-20 01:15:40,506 - __main__ - DEBUG - 90% of data are between -0.707292950153 and 0.297696024179, thresholding the rest
2017-05-20 01:15:40,506 - __main__ - DEBUG - Processing feature 3
2017-05-20 01:15:42,213 - __main__ - DEBUG - 90% of data are between -1.08608081341 and 0.171938210726, thresholding the rest
2017-05-20 01:15:42,571 - __main__ - DEBUG - 90% of data are between -0.821817216277 and 0.166289664805, thresholding the rest
2017-05-20 01:15:42,571 - __main__ - DEBUG - Processing feature 4
2017-05-20 01:15:44,523 - __main__ - DEBUG - 90% of data are between -0.927134382725 and 0.290628248453, thresholding the rest
2017-05-20 01:15:44,925 - __main__ - DEBUG - 90% of data are between -0.810377755761 and 0.17299163565, thresholding the rest
2017-05-20 01:15:44,925 - __main__ - DEBUG - Processing feature 5
2017-05-20 01:15:47,114 - __main__ - DEBUG - 90% of data are between -0.930253624916 and 0.294768428802, thresholding the rest
2017-05-20 01:15:47,555 - __main__ - DEBUG - 90% of data are between -0.890853658319 and 0.150400905311, thresholding the rest
2017-05-20 01:15:47,556 - __main__ - DEBUG - Processing feature 6
2017-05-20 01:15:49,949 - __main__ - DEBUG - 90% of data are between -0.764237046242 and 0.428253990412, thresholding the rest
2017-05-20 01:15:50,438 - __main__ - DEBUG - 90% of data are between -0.542945283651 and 0.448660154641, thresholding the rest
2017-05-20 01:15:50,439 - __main__ - DEBUG - Processing feature 7
2017-05-20 01:15:53,065 - __main__ - DEBUG - 90% of data are between -0.743733203411 and 0.534563958645, thresholding the rest
2017-05-20 01:15:53,636 - __main__ - DEBUG - 90% of data are between -0.486769029498 and 0.538608041406, thresholding the rest
2017-05-20 01:15:53,637 - __main__ - DEBUG - Processing feature 8
2017-05-20 01:15:56,600 - __main__ - DEBUG - 90% of data are between -0.401596236229 and 0.841694891453, thresholding the rest
2017-05-20 01:15:57,479 - __main__ - DEBUG - 90% of data are between -0.294111338258 and 0.696886143088, thresholding the rest
2017-05-20 01:15:57,479 - __main__ - DEBUG - Processing feature 9
2017-05-20 01:16:02,193 - __main__ - DEBUG - 90% of data are between -0.270120632648 and 0.995177853107, thresholding the rest
2017-05-20 01:16:02,817 - __main__ - DEBUG - 90% of data are between -0.130249094963 and 0.889141267538, thresholding the rest
2017-05-20 01:16:02,817 - __main__ - DEBUG - Processing feature 10
2017-05-20 01:16:06,140 - __main__ - DEBUG - 90% of data are between -1.02727777958 and 0.242912685871, thresholding the rest
2017-05-20 01:16:06,791 - __main__ - DEBUG - 90% of data are between -0.935506621003 and 0.0449944617227, thresholding the rest
2017-05-20 01:16:06,791 - __main__ - DEBUG - Processing feature 11
2017-05-20 01:16:10,352 - __main__ - DEBUG - 90% of data are between -0.458769887686 and 0.820109784603, thresholding the rest
2017-05-20 01:16:11,059 - __main__ - DEBUG - 90% of data are between -0.233166665584 and 0.754398557544, thresholding the rest
2017-05-20 01:16:11,059 - __main__ - DEBUG - Processing feature 12
2017-05-20 01:16:14,764 - __main__ - DEBUG - 90% of data are between -0.613964951038 and 0.663808977604, thresholding the rest
2017-05-20 01:16:15,536 - __main__ - DEBUG - 90% of data are between -0.506987392902 and 0.505788505077, thresholding the rest
2017-05-20 01:16:15,536 - __main__ - DEBUG - Processing feature 13
2017-05-20 01:16:19,454 - __main__ - DEBUG - 90% of data are between -0.703447508812 and 0.582039773464, thresholding the rest
2017-05-20 01:16:20,261 - __main__ - DEBUG - 90% of data are between -0.490731106699 and 0.606320211291, thresholding the rest
2017-05-20 01:16:20,261 - __main__ - DEBUG - Processing feature 14
2017-05-20 01:16:24,677 - __main__ - DEBUG - 90% of data are between -0.0623662896454 and 1.13466579914, thresholding the rest
2017-05-20 01:16:26,174 - __main__ - DEBUG - 90% of data are between -0.16601280719 and 0.843919727206, thresholding the rest
2017-05-20 01:16:26,174 - __main__ - DEBUG - Processing feature 15
2017-05-20 01:16:33,749 - __main__ - DEBUG - 90% of data are between -1.0403870821 and 0.228742572665, thresholding the rest
2017-05-20 01:16:34,657 - __main__ - DEBUG - 90% of data are between -0.753181552887 and 0.259445467591, thresholding the rest
2017-05-20 01:16:34,657 - __main__ - DEBUG - Processing feature 16
2017-05-20 01:16:39,327 - __main__ - DEBUG - 90% of data are between -0.530158114433 and 0.741996669769, thresholding the rest
2017-05-20 01:16:40,240 - __main__ - DEBUG - 90% of data are between -0.412233386934 and 0.596443909407, thresholding the rest
2017-05-20 01:16:40,240 - __main__ - DEBUG - Processing feature 17
2017-05-20 01:16:45,228 - __main__ - DEBUG - 90% of data are between -0.906278371811 and 0.31312020421, thresholding the rest
2017-05-20 01:16:46,290 - __main__ - DEBUG - 90% of data are between -0.723698821664 and 0.289864361286, thresholding the rest
2017-05-20 01:16:46,290 - __main__ - DEBUG - Processing feature 18
2017-05-20 01:16:51,518 - __main__ - DEBUG - 90% of data are between -0.578585088253 and 0.676933026314, thresholding the rest
2017-05-20 01:16:52,641 - __main__ - DEBUG - 90% of data are between -0.417234261334 and 0.602287116647, thresholding the rest
2017-05-20 01:16:52,641 - __main__ - DEBUG - Processing feature 19
2017-05-20 01:16:59,708 - __main__ - DEBUG - 90% of data are between -0.19328109026 and 1.01468822956, thresholding the rest
2017-05-20 01:17:01,729 - __main__ - DEBUG - 90% of data are between -0.258490748703 and 0.727056634426, thresholding the rest
2017-05-20 01:17:01,729 - __main__ - DEBUG - Processing feature 20
2017-05-20 01:17:10,738 - __main__ - DEBUG - 90% of data are between -0.622477090359 and 0.604018783569, thresholding the rest
2017-05-20 01:17:11,855 - __main__ - DEBUG - 90% of data are between -0.617208108306 and 0.391470164061, thresholding the rest
2017-05-20 01:17:11,855 - __main__ - DEBUG - Processing feature 21
2017-05-20 01:17:17,675 - __main__ - DEBUG - 90% of data are between -0.575907492638 and 0.67855206728, thresholding the rest
2017-05-20 01:17:18,860 - __main__ - DEBUG - 90% of data are between -0.350522018969 and 0.672930219769, thresholding the rest
2017-05-20 01:17:18,860 - __main__ - DEBUG - Processing feature 22
2017-05-20 01:17:25,003 - __main__ - DEBUG - 90% of data are between -0.709619355202 and 0.501450073719, thresholding the rest
2017-05-20 01:17:26,205 - __main__ - DEBUG - 90% of data are between -0.677170681953 and 0.357698974013, thresholding the rest
2017-05-20 01:17:26,205 - __main__ - DEBUG - Processing feature 23
2017-05-20 01:17:32,993 - __main__ - DEBUG - 90% of data are between -0.69191699028 and 0.539990520477, thresholding the rest
2017-05-20 01:17:35,323 - __main__ - DEBUG - 90% of data are between -0.668328499794 and 0.335854536295, thresholding the rest
2017-05-20 01:17:35,323 - __main__ - DEBUG - Processing feature 24
2017-05-20 01:17:47,607 - __main__ - DEBUG - 90% of data are between -0.339375221729 and 0.886764705181, thresholding the rest
2017-05-20 01:17:49,984 - __main__ - DEBUG - 90% of data are between -0.328031592071 and 0.648147121072, thresholding the rest
2017-05-20 01:17:49,984 - __main__ - DEBUG - Processing feature 25
2017-05-20 01:17:58,221 - __main__ - DEBUG - 90% of data are between -0.683309018612 and 0.569608855247, thresholding the rest
2017-05-20 01:17:59,549 - __main__ - DEBUG - 90% of data are between -0.588562017679 and 0.436539471149, thresholding the rest
2017-05-20 01:17:59,549 - __main__ - DEBUG - Processing feature 26
2017-05-20 01:18:06,583 - __main__ - DEBUG - 90% of data are between -0.349127590656 and 0.945802640915, thresholding the rest
2017-05-20 01:18:07,966 - __main__ - DEBUG - 90% of data are between -0.168173283339 and 0.986967939138, thresholding the rest
2017-05-20 01:18:07,966 - __main__ - DEBUG - Processing feature 27
2017-05-20 01:18:15,537 - __main__ - DEBUG - 90% of data are between -0.753855359554 and 0.475349205732, thresholding the rest
2017-05-20 01:18:17,066 - __main__ - DEBUG - 90% of data are between -0.540736410022 and 0.44476442039, thresholding the rest
2017-05-20 01:18:17,066 - __main__ - DEBUG - Processing feature 28
2017-05-20 01:18:31,229 - __main__ - DEBUG - 90% of data are between -0.735401153564 and 0.501543855667, thresholding the rest
2017-05-20 01:18:33,984 - __main__ - DEBUG - 90% of data are between -0.599398195744 and 0.387410701811, thresholding the rest
2017-05-20 01:18:33,984 - __main__ - DEBUG - Processing feature 29
2017-05-20 01:18:43,191 - __main__ - DEBUG - 90% of data are between -0.29808909297 and 0.926633250713, thresholding the rest
2017-05-20 01:18:44,701 - __main__ - DEBUG - 90% of data are between -0.280193188787 and 0.71093467474, thresholding the rest
2017-05-20 01:18:44,701 - __main__ - DEBUG - Processing feature 30
2017-05-20 01:18:52,763 - __main__ - DEBUG - 90% of data are between -0.289808046818 and 0.950104355812, thresholding the rest
2017-05-20 01:18:54,327 - __main__ - DEBUG - 90% of data are between -0.121881852672 and 0.882012581825, thresholding the rest
2017-05-20 01:18:54,327 - __main__ - DEBUG - Processing feature 31
2017-05-20 01:19:04,460 - __main__ - DEBUG - 90% of data are between -0.231506171823 and 0.983058512211, thresholding the rest
2017-05-20 01:19:07,509 - __main__ - DEBUG - 90% of data are between -0.105051603541 and 0.834137219191, thresholding the rest
2017-05-20 01:19:07,509 - __main__ - DEBUG - Processing feature 32
2017-05-20 01:19:23,500 - __main__ - DEBUG - 90% of data are between -0.133253675699 and 1.07980465889, thresholding the rest
2017-05-20 01:19:25,930 - __main__ - DEBUG - 90% of data are between 0.00226386323338 and 1.02542737126, thresholding the rest
2017-05-20 01:19:25,930 - __main__ - DEBUG - Processing feature 33
2017-05-20 01:19:34,769 - __main__ - DEBUG - 90% of data are between -0.291458529234 and 0.915519881248, thresholding the rest
2017-05-20 01:19:36,498 - __main__ - DEBUG - 90% of data are between -0.250270144641 and 0.791211658716, thresholding the rest
2017-05-20 01:19:36,499 - __main__ - DEBUG - Processing feature 34
2017-05-20 01:19:45,509 - __main__ - DEBUG - 90% of data are between -0.559727525711 and 0.695286607742, thresholding the rest
2017-05-20 01:19:47,331 - __main__ - DEBUG - 90% of data are between -0.577225464582 and 0.462761713564, thresholding the rest
2017-05-20 01:19:47,331 - __main__ - DEBUG - Processing feature 35
2017-05-20 01:20:03,383 - __main__ - DEBUG - 90% of data are between -0.820211291313 and 0.37860506773, thresholding the rest
2017-05-20 01:20:06,725 - __main__ - DEBUG - 90% of data are between -0.70477193892 and 0.294070823491, thresholding the rest
2017-05-20 01:20:06,725 - __main__ - DEBUG - Processing feature 36
2017-05-20 01:20:19,124 - __main__ - DEBUG - 90% of data are between -0.883394062519 and 0.366400241852, thresholding the rest
2017-05-20 01:20:21,011 - __main__ - DEBUG - 90% of data are between -0.846879982948 and 0.170039347559, thresholding the rest
2017-05-20 01:20:21,012 - __main__ - DEBUG - Processing feature 37
2017-05-20 01:20:30,894 - __main__ - DEBUG - 90% of data are between -0.200877732038 and 1.0226654768, thresholding the rest
2017-05-20 01:20:32,809 - __main__ - DEBUG - 90% of data are between -0.166700640321 and 0.87782626152, thresholding the rest
2017-05-20 01:20:32,810 - __main__ - DEBUG - Processing feature 38
2017-05-20 01:20:47,860 - __main__ - DEBUG - 90% of data are between -0.348761248589 and 0.932293462753, thresholding the rest
2017-05-20 01:20:51,466 - __main__ - DEBUG - 90% of data are between -0.245736000687 and 0.776645451784, thresholding the rest
2017-05-20 01:20:51,467 - __main__ - DEBUG - Processing feature 39
2017-05-20 01:21:06,350 - __main__ - DEBUG - 90% of data are between -0.942997169495 and 0.27925888896, thresholding the rest
2017-05-20 01:21:08,324 - __main__ - DEBUG - 90% of data are between -0.779672059417 and 0.201474422961, thresholding the rest
2017-05-20 01:21:08,325 - __main__ - DEBUG - Done in 332.579257 seconds
2017-05-20 01:21:08,325 - __main__ - DEBUG - Computing trimmed predictions
2017-05-20 01:21:08,325 - __main__ - DEBUG - Making trimmed features predictions
Traceback (most recent call last):
  File "/mnt/encrypted_data/Dropbox/cmu/foundation_of_privacy/project/fop_qii/MovieLensALS.py", line 491, in <module>
    main()
  File "/mnt/encrypted_data/Dropbox/cmu/foundation_of_privacy/project/fop_qii/MovieLensALS.py", line 435, in main
    train_ratio=args.cross_validation)
  File "/mnt/encrypted_data/Dropbox/cmu/foundation_of_privacy/project/fop_qii/internal_feature_predictor.py", line 1020, in internal_feature_predictor
    baseline_predictions)
  File "/mnt/encrypted_data/Dropbox/cmu/foundation_of_privacy/project/fop_qii/internal_feature_predictor.py", line 765, in load_or_train_trimmed_recommender
    trimmed_predictions = model.predictAll(training)
  File "/mnt/encrypted_data/Dropbox/cmu/foundation_of_privacy/project/fop_qii/TrimmedFeatureRecommender.py", line 97, in predictAll
    self.p_feats)
  File "/mnt/encrypted_data/Dropbox/cmu/foundation_of_privacy/project/fop_qii/common_utils.py", line 161, in manual_predict_all
    user_features = dict(user_features.collect())
  File "/opt/spark-2.0.1/python/lib/pyspark.zip/pyspark/rdd.py", line 776, in collect
  File "/opt/spark-2.0.1/python/lib/pyspark.zip/pyspark/rdd.py", line 2403, in _jrdd
  File "/opt/spark-2.0.1/python/lib/pyspark.zip/pyspark/rdd.py", line 2336, in _wrap_function
  File "/opt/spark-2.0.1/python/lib/pyspark.zip/pyspark/rdd.py", line 2315, in _prepare_for_python_RDD
  File "/opt/spark-2.0.1/python/lib/pyspark.zip/pyspark/serializers.py", line 428, in dumps
  File "/opt/spark-2.0.1/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 657, in dumps
  File "/opt/spark-2.0.1/python/lib/pyspark.zip/pyspark/cloudpickle.py", line 111, in dump
pickle.PicklingError: Could not pickle object as excessively deep recursion required.
memory: 1,117,878
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,119,683
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,032B for [id] INT32: 3,747 values, 14,988B raw, 14,994B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 971,942
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 906,364B for [features, list, element] DOUBLE: 150,320 values, 1,217,617B raw, 906,269B comp, 2 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 14,691B for [id] INT32: 3,662 values, 14,648B raw, 14,653B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 15,291B for [id] INT32: 3,812 values, 15,248B raw, 15,253B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 899,815B for [features, list, element] DOUBLE: 149,880 values, 1,214,053B raw, 899,720B comp, 2 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 884,121B for [features, list, element] DOUBLE: 146,480 values, 1,186,513B raw, 884,026B comp, 2 pages, encodings: [RLE, PLAIN]
May 20, 2017 12:44:49 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 914,422B for [features, list, element] DOUBLE: 152,480 values, 1,235,113B raw, 914,327B comp, 2 pages, encodings: [RLE, PLAIN]
